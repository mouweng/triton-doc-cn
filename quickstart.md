# 快速开始

想要快速部署你的模型吗？Triton Inference Server的新手！利用下面的[教程](https://github.com/triton-inference-server/tutorials#quick-deploy)开始你的Triton之旅吧！

Triton Inference Server可以使用源码进行构建，但安装和运行Triton的最简单方法是使用[NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/)提供的已经构建好的Docker映像。

运行和维护Triton需要基于构建模型仓库。本小结的教程包括如下部分：

- 创建一个模型仓库
- 运行Triton
- 向Triton发送推理请求

## 创建一个模型仓库