# 模型存储库

这是你第一次设置模型存储库吗？请参考这个的[教程](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_1-model_deployment#setting-up-the-model-repository)开启你的Trion之旅吧！

Triton推理服务器从一个或多个模型存储库中提供模型，这些存储库在启动服务器时进行指定。在Triton运行时，可以根据模型管理中的描述对正在提供的模型进行修改。

## 存储库结构

这些存储库路径是在启动Triton时使用"--model-repository"选项指定的。"--model-repository"选项可以多次指定，以包含来自多个存储库的模型。构成模型存储库的目录和文件必须遵循一种必要的布局。假设指定了一个存储库路径，如下所示。

```shell
tritonserver --model-repository=<model-repository-path>
```

对应的存储库布局必须是:

````xml
 <model-repository-path>/
    <model-name>/
      [config.pbtxt]
      [<output-labels-file> ...]
      <version>/
        <model-definition-file>
      <version>/
        <model-definition-file>
      ...
    <model-name>/
      [config.pbtxt]
      [<output-labels-file> ...]
      <version>/
        <model-definition-file>
      <version>/
        <model-definition-file>
      ...
    ...
````

在最顶层的模型存储库目录中，可以有0个或多个子目录。每个子目录包含相应模型的存储库信息。config.pbtxt文件描述了模型的[模型配置](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html)。对于某些模型，config.pbtxt是必需的，而对于其他模型，它是可选的。有关更多信息，请参阅[自动生成的模型配置](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#auto-generated-model-configuration)。

每个目录必须至少有一个数字子目录，代表模型的一个版本。有关Triton如何处理每个目录必须至少有一个表示模型版本的数字子目录。有关Triton如何处理模型版本的更多信息，请参阅[模型版本](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html#model-versions)。每个模型由特定的[Backend](https://github.com/triton-inference-server/backend/blob/main/README.md)执行。在每个版本的子目录中，必须包含该Backend所需的文件。例如，使用TensorRT、PyTorch、ONNX、OpenVINO和TensorFlow等框架后端的模型必须提供特定于框架的模型文件。

## 模型存储库位置

Triton可以从一个或多个本地可访问的文件路径、Google Cloud Storage、Amazon S3和Azure Storage中访问模型。

### 本地文件系统

对于本地可访问的文件系统，必须指定绝对路径。

```
$ tritonserver --model-repository=/path/to/model/repository ...
```

### 云存储使用环境变量进行配置

#### Google Cloud Storage

对于存储在Google Cloud Storage中的模型存储库，存储库路径必须以"gs://"为前缀。

```shell
$ tritonserver --model-repository=gs://bucket/path/to/model/repository ...
```

在使用Google Cloud Storage时，应设置`GOOGLE_APPLICATION_CREDENTIALS`环境变量，并包含凭证JSON文件的位置。如果未提供凭证，Triton将使用附加的服务帐号提供Authorization HTTP标头值。如果无法获取该值，则将使用匿名凭证。

要使用匿名凭证访问存储桶（也称为公共存储桶），该存储桶（及其对象）应授予所有用户获取和列表权限。经过测试，向存储桶添加`storage.objectViewer`和`storage.legacyBucketReader`预定义角色，授予`allUsers`的权限可以实现该目的。

```shell
$ gsutil iam ch allUsers:objectViewer "${BUCKET_URL}"
$ gsutil iam ch allUsers:legacyBucketReader "${BUCKET_URL}"
```

默认情况下，Triton会在临时文件夹中创建一个远程模型存储库的本地副本，并在Triton服务器关闭后将其删除。如果您想要控制远程模型存储库的复制位置，可以将`TRITON_GCS_MOUNT_DIRECTORY`环境变量设置为指向本地机器上现有文件夹的路径。

```shell
export TRITON_GCS_MOUNT_DIRECTORY=/path/to/your/local/directory
```

请确保 `TRITON_GCS_MOUNT_DIRECTORY` 在您的本地计算机上存在，并且是一个空文件夹。

#### S3

对于存储在Amazon S3中的模型存储库，路径必须以"s3://"为前缀。

```shell
$ tritonserver --model-repository=as://account_name/container_name/path/to/model/repository ...
```

对于使用本地或私有的S3实例，前缀"s3://"后面必须跟着主机和端口（用分号分隔），然后是存储桶路径。

```shell
$ tritonserver --model-repository=s3://host:port/bucket/path/to/model/repository ...
```

默认情况下，Triton使用HTTP与您的S3实例通信。如果您的S3实例支持HTTPS，并且希望Triton使用HTTPS协议与其通信，您可以在模型存储库路径中使用`https://`前缀来指定。

```shell
$ tritonserver --model-repository=s3://https://host:port/bucket/path/to/model/repository ...
```

在使用S3时，可以通过使用aws config命令或相应的环境变量传递凭证和默认区域。如果设置了环境变量，它们将优先使用，并且Triton将使用这些环境变量设置的凭证，而不是使用aws config命令设置的凭证。

默认情况下，Triton会在临时文件夹中创建远程模型存储库的本地副本，并在Triton服务器关闭后删除该副本。如果您希望控制远程模型存储库的复制位置，可以将`TRITON_AWS_MOUNT_DIRECTORY`环境变量设置为指向本地计算机上现有文件夹的路径。

```shell
export TRITON_AWS_MOUNT_DIRECTORY=/path/to/your/local/directory
```

请确保`TRITON_AWS_MOUNT_DIRECTORY`存在于您的本地计算机上，并且是一个空文件夹。

#### Azure Storage

对于存储在Azure存储中的模型存储库，存储库路径必须以"as://"为前缀。

```shell
$ tritonserver --model-repository=as://account_name/container_name/path/to/model/repository ...
```

在使用Azure存储时，您必须将`AZURE_STORAGE_ACCOUNT`和`AZURE_STORAGE_KEY`环境变量设置为具有访问Azure存储库权限的帐户。

如果您不知道`AZURE_STORAGE_KEY`并且已正确配置Azure CLI，以下是一个示例，演示如何找到与您的`AZURE_STORAGE_ACCOUNT`对应的密钥：

```shell
$ export AZURE_STORAGE_ACCOUNT="account_name"
$ export AZURE_STORAGE_KEY=$(az storage account keys list -n $AZURE_STORAGE_ACCOUNT --query "[0].value")
```

默认情况下，Triton会在临时文件夹中创建远程模型存储库的本地副本，并在Triton服务器关闭后删除该副本。如果您希望控制远程模型存储库的复制位置，可以将`TRITON_AZURE_MOUNT_DIRECTORY`环境变量设置为指向本地计算机上现有文件夹的路径。

```shell
export TRITON_AZURE_MOUNT_DIRECTORY=/path/to/your/local/directory
```

请确保 TRITON_AZURE_MOUNT_DIRECTORY 在您的本地计算机上存在，并且是一个空文件夹。

### 云存储使用凭证文件

> 此功能目前处于测试阶段，可能会进行变更。

用于将凭据分组到一个JSON文件中,您可以设置TRITON_CLOUD_CREDENTIAL_PATH环境变量，指向位于本地文件系统中的以下格式的JSON文件。

```shell
export TRITON_CLOUD_CREDENTIAL_PATH="cloud_credential.json"
```

“cloud_credential.json”:

```shell
{
  "gs": {
    "": "PATH_TO_GOOGLE_APPLICATION_CREDENTIALS",
    "gs://gcs-bucket-002": "PATH_TO_GOOGLE_APPLICATION_CREDENTIALS_2"
  },
  "s3": {
    "": {
      "secret_key": "AWS_SECRET_ACCESS_KEY",
      "key_id": "AWS_ACCESS_KEY_ID",
      "region": "AWS_DEFAULT_REGION",
      "session_token": "",
      "profile": ""
    },
    "s3://s3-bucket-002": {
      "secret_key": "AWS_SECRET_ACCESS_KEY_2",
      "key_id": "AWS_ACCESS_KEY_ID_2",
      "region": "AWS_DEFAULT_REGION_2",
      "session_token": "AWS_SESSION_TOKEN_2",
      "profile": "AWS_PROFILE_2"
    }
  },
  "as": {
    "": {
      "account_str": "AZURE_STORAGE_ACCOUNT",
      "account_key": "AZURE_STORAGE_KEY"
    },
    "as://Account-002/Container": {
      "account_str": "",
      "account_key": ""
    }
  }
}
```

在该文件中，您可以为不同的云存储提供商指定凭据。为了匹配凭据，Triton使用给定路径的最长匹配凭据名称。例如，路径`gs://gcs-bucket-002/model_repository`将匹配"GCS"凭据中的`gs://gcs-bucket-002`，而路径`gs://any-other-gcs-bucket`将匹配"GCS"凭据中的""（空凭据）。

请确保将示例中的凭据路径/密钥替换为实际的路径/密钥。

如果未设置`TRITON_CLOUD_CREDENTIAL_PATH`环境变量，则将使用**Cloud Storage with Environment variables**（使用环境变量）的方式进行访问云存储。

### 云存储本地缓存

Triton目前不对云存储执行文件缓存。然而，通过在存储库代理API中注入一个代理，可以实现此功能。该代理会检查特定的本地目录以进行缓存，根据模型的云存储（原始路径）来决定是否可以使用缓存文件。

## 模型版本

每个模型在模型仓库中可以有一个或多个可用版本。每个版本都存储在自己的数字命名的子目录中，子目录的名称与模型的版本号对应。那些没有数字命名或以零（0）开头的子目录将被忽略。每个模型配置都指定了一个[版本策略](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#version-policy)，该策略控制 Triton 在任何给定时间内提供模型仓库中的哪些版本。

## 模型文件

每个模型版本子目录的内容取决于模型的类型和支持该模型的后端的要求。

### TensorRT Models

TensorRT模型定义被称为Plan。TensorRT Plan是一个单独的文件，默认情况下必须命名为model.plan。可以使用模型配置中的`default_model_filename`属性来覆盖此默认名称。

TensorRT Plan专用于GPU的CUDA计算能力。因此，TensorRT模型需要在模型配置中设置`cc_model_filenames`属性，将每个Plan文件与相应的计算能力关联起来。

TensorRT模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.plan
```

### ONNX Models

ONNX模型可以是单个文件或包含多个文件的目录。默认情况下，文件或目录必须命名为model.onnx。可以使用模型配置中的`default_model_filename`属性来覆盖此默认名称。

Triton支持由Triton使用的ONNX Runtime版本支持的所有ONNX模型。如果模型使用过时的ONNX opset版本或包含不受支持类型的运算符，则不支持该模型。

包含在单个文件中的ONNX模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx
```

由多个文件组成的ONNX模型必须包含在一个目录中。默认情况下，此目录必须命名为model.onnx，但可以使用模型配置中的`default_model_filename`属性进行覆盖。此目录中的主要模型文件必须命名为model.onnx。

包含在目录中的ONNX模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx/
           model.onnx
           <other model files>
```

### TorchScript Models

TorchScript模型是一个单独的文件，默认情况下必须命名为model.pt。可以使用模型配置中的default_model_filename属性来覆盖此默认名称。由于底层操作集发生了变化，使用不同版本的PyTorch跟踪的某些模型可能不受Triton支持。

TorchScript模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.pt
```

### TensorFlow Models

TensorFlow以两种格式保存模型：GraphDef和SavedModel。Triton支持这两种格式。

TensorFlow GraphDef是一个单独的文件，默认情况下必须命名为model.graphdef。TensorFlow SavedModel是一个包含多个文件的目录。默认情况下，目录必须命名为model.savedmodel。可以使用模型配置中的default_model_filename属性来覆盖这些默认名称。

TensorFlow GraphDef模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.graphdef
```

TensorFlow SavedModel模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.savedmodel/
           <saved-model files>
```

### OpenVINO Models

OpenVINO模型由两个文件表示，一个是*.xml文件，另一个是*.bin文件。默认情况下，*.xml文件必须命名为model.xml。可以使用模型配置中的default_model_filename属性来覆盖此默认名称。

OpenVINO模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.xml
        model.bin
```

### Python Models

Python后端允许您在Triton中运行Python代码作为模型。默认情况下，Python脚本必须命名为model.py，但可以使用模型配置中的default_model_filename属性进行覆盖。

Python模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.py
```

### DALI Models

DALI后端允许您在Triton中将DALI流水线作为模型运行。为了使用这个后端，您需要生成一个名为model.dali的文件，并将其包含在您的模型仓库中。请参考DALI后端文档以了解如何生成model.dali。可以使用模型配置中的`default_model_filename`属性来覆盖默认的模型文件名。

DALI模型的最小模型仓库结构如下：

```xml
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.dali
```
