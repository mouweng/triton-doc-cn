# 快速开始

想要快速部署你的模型吗？Triton Inference Server的新手！利用下面的[教程](https://github.com/triton-inference-server/tutorials#quick-deploy)开始你的Triton之旅吧！

Triton Inference Server可以使用源码进行构建，但安装和运行Triton的最简单方法是使用[NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/)提供的已经构建好的Docker映像。

运行和维护Triton需要基于构建模型存储库。本小结的教程包括如下部分：

- 创建一个模型存储库
- 运行Triton
- 向Triton发送推理请求

## 创建一个模型存储库

模型存储库是你放置模型的目录，如果你希望使用Triton来部署这些模型。[文档](https://github.com/triton-inference-server/server/tree/main/docs/examples)的`docs/examples/model_repository`目录提供了一个示例的模型存储库。在使用这个存储库之前，你需要根据如下脚本从公共的模型Zoos中获取缺失的部分模型：

```shell
$ cd docs/examples
$ ./fetch_models.sh
```

## 启动Triton

Triton使用GPU进行优化来提供最好的推理性能，但Triton也可以在只有CPU的机器上运行，这两种情况可以使用同一个Triton Docker镜像。

### 在GPU的机器上运行

使用下面的命令，通过Triton来部署你上面创建的模型存储库。如果要让Docker识别GPU，必须安装[NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)。`--gpus=1`表示使用1个GPU来做推理。

```shell
$ docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
```

 `<xx.yy>`表示你想使用的Triton版本。在你启动Triton之后，你将会在控制台上看到如下的输出，表示服务已经启动并成功加载了模型，Triton已经可以接收推理的请求：

```shell
+----------------------+---------+--------+
| Model                | Version | Status |
+----------------------+---------+--------+
| <model_name>         | <v>     | READY  |
| ..                   | .       | ..     |
| ..                   | .       | ..     |
+----------------------+---------+--------+
...
...
...
I1002 21:58:57.891440 62 grpc_server.cc:3914] Started GRPCInferenceService at 0.0.0.0:8001
I1002 21:58:57.893177 62 http_server.cc:2717] Started HTTPService at 0.0.0.0:8000
I1002 21:58:57.935518 62 http_server.cc:2736] Started Metrics Service at 0.0.0.0:8002
```

所有的模型都应该显示`READY`状态，表明它们被正确地加载。如果其中有模型加载失败，Triton将会打印出失败的状态以及失败的具体原因。如果模型没有显示在表中，请检查模型存储库的路径和CUDA驱动程序。

### 在CPU的机器上运行

在没有GPU的机器上，Trion不需要使用`--gpus`参数，其他参数的配置和上述一致。

```shell
$ docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models
```

由于`--gpus`参数没有开启，GPU不可用，因此Triton将无法加载任何需要GPU的模型配置。

### 验证Triton是否正常运行

使用Triton的`ready`接口来验证服务是否已经成功加载模型。使用curl访问HTTP接口。

```shell
$ curl -v localhost:8000/v2/health/ready
...
< HTTP/1.1 200 OK
< Content-Length: 0
< Content-Type: text/plain
```

正常情况下，HTTP请求返回状态200，否则服务为不正常。

### 发送推理请求

使用docker拉取客户端镜像。

```shell
$ docker pull nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk
```

`<xx.yy>`表示你想要拉取的具体版本，运行这个镜像。

```shell
$ docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:<xx.yy>-py3-sdk
```

在镜像中，运行`image-client`示例程序，请求部署好的`densenet_onnx`模型，实现图像分类效果。

请使用`/workspace/images`目录中的图片给`densenet_onnx`模型发送请求，在本示例中，我们获取排名前3的分类。

```shell
$ /workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg
Request 0, batch size 1
Image '/workspace/images/mug.jpg':
    15.346230 (504) = COFFEE MUG
    13.224326 (968) = CUP
    10.422965 (505) = COFFEEPOT
```

